"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"ALHBT2Z8","preprint","2018","Zhang, Chuxu; Song, Dongjin; Chen, Yuncong; Feng, Xinyang; Lumezanu, Cristian; Cheng, Wei; Ni, Jingchao; Zong, Bo; Chen, Haifeng; Chawla, Nitesh V.","A Deep Neural Network for Unsupervised Anomaly Detection and Diagnosis in Multivariate Time Series Data","","","","https://doi.org/10.1609/aaai.v33i01.33011409","http://arxiv.org/abs/1811.08055","Nowadays, multivariate time series data are increasingly collected in various real world systems, e.g., power plants, wearable devices, etc. Anomaly detection and diagnosis in multivariate time series refer to identifying abnormal status in certain time steps and pinpointing the root causes. Building such a system, however, is challenging since it not only requires to capture the temporal dependency in each time series, but also need encode the inter-correlations between different pairs of time series. In addition, the system should be robust to noise and provide operators with different levels of anomaly scores based upon the severity of different incidents. Despite the fact that a number of unsupervised anomaly detection algorithms have been developed, few of them can jointly address these challenges. In this paper, we propose a Multi-Scale Convolutional Recurrent Encoder-Decoder (MSCRED), to perform anomaly detection and diagnosis in multivariate time series data. Specifically, MSCRED first constructs multi-scale (resolution) signature matrices to characterize multiple levels of the system statuses in different time steps. Subsequently, given the signature matrices, a convolutional encoder is employed to encode the inter-sensor (time series) correlations and an attention based Convolutional Long-Short Term Memory (ConvLSTM) network is developed to capture the temporal patterns. Finally, based upon the feature maps which encode the inter-sensor correlations and temporal information, a convolutional decoder is used to reconstruct the input signature matrices and the residual signature matrices are further utilized to detect and diagnose anomalies. Extensive empirical studies based on a synthetic dataset and a real power plant dataset demonstrate that MSCRED can outperform state-of-the-art baseline methods.","2018-11-19","2024-03-02 10:29:12","2025-05-25 10:55:54","2024-03-02 10:29:12","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1811.08055 [cs, stat]","","/Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/ZKBDNAEE/Zhang et al. - 2018 - A Deep Neural Network for Unsupervised Anomaly Det.pdf; /Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/ZAXTGWWN/1811.html; ","notion://www.notion.so/Zhang-et-al-2018-37fab1c18b3b4103991759537fde5d7d","Anomaly detection; Autoencoder; Classified; Deep Learning; Fault classification; NN; notion; Read","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1811.08055","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8EQIGVE8","conferencePaper","2017","Zhou, Chong; Paffenroth, Randy C.","Anomaly Detection with Robust Deep Autoencoders","Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","978-1-4503-4887-4","","10.1145/3097983.3098052","https://dl.acm.org/doi/10.1145/3097983.3098052","Deep autoencoders, and other deep neural networks, have demonstrated their e ectiveness in discovering non-linear features across many problem domains. However, in many real-world problems, large outliers and pervasive noise are commonplace, and one may not have access to clean training data as required by standard deep denoising autoencoders. Herein, we demonstrate novel extensions to deep autoencoders which not only maintain a deep autoencoders’ ability to discover high quality, non-linear features but can also eliminate outliers and noise without access to any clean training data. Our model is inspired by Robust Principal Component Analysis, and we split the input data X into two parts, X = LD + S, where LD can be e ectively reconstructed by a deep autoencoder and S contains the outliers and noise in the original data X . Since such spli ing increases the robustness of standard deep autoencoders, we name our model a “Robust Deep Autoencoder (RDA)”. Further, we present generalizations of our results to grouped sparsity norms which allow one to distinguish random anomalies from other types of structured corruptions, such as a collection of features being corrupted across many instances or a collection of instances having more corruptions than their fellows. Such “Group Robust Deep Autoencoders (GRDA)” give rise to novel anomaly detection approaches whose superior performance we demonstrate on a selection of benchmark problems.","2017-08-04","2024-03-07 17:47:29","2025-05-25 10:56:36","2024-03-07 17:47:29","665-674","","","","","","","","","","","ACM","Halifax NS Canada","en","","","","","DOI.org (Crossref)","","","<div data-schema-version=""8""><p><span style=""color: rgb(31, 31, 31)""><span style=""background-color: rgb(255, 255, 255)"">Dissertation on training Autoencoders for Anomaly Detection. It tries to make them robust by regularization and training in multiple directions (adapting training with different regularizations per steps). Well explained piece about autoencoders. Uses MNIST 2D data as example.</span></span></p> <p>Similar to was already done to Carriage data!!! Iteratively detect and take out anomalous from training data</p> </div>","; /Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/WYRWPYN5/Zhou and Paffenroth - 2017 - Anomaly Detection with Robust Deep Autoencoders.pdf","notion://www.notion.so/Zhou-y-Paffenroth-2017-3d4b379792e444018627848c5acb8c6b","2D input; Anomaly detection; Autoencoder; Classified; Clean noise in training data; Good Autoencoder theory; NN; notion; Read","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","KDD '17: The 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","","","","","","","","","","","","","","",""
"ZL886D29","journalArticle","2021","Hu, Zhongfeng; Huang, Xiaodi","A Novel Vehicle Gearbox Fault Diagnosis Approach Based on Collective Anomaly Detection","E3S Web of Conferences","","2267-1242","10.1051/e3sconf/202125201052","https://www.e3s-conferences.org/10.1051/e3sconf/202125201052","Targeting the problem of gearbox fault diagnosis, we proposed a novel semi-supervised approach based on collective anomaly detection. Based on the limited sample data, the principle of the approach is to detect whether a test dataset contains abnormal patterns by using data distribution as the metric. The sequence obeying unexpected distribution will be identified as collective anomaly, which may be generated by fault patterns. The approach consists of three steps. First, the mixture of multivariate Gaussian distribution is used to fit the structure of sample dataset and test dataset. Then, based on maximum likelihood estimate algorithm, we hope to search the optimal parameters which can fit the data distribution with the highest degree. Finally, the fixed point iteration algorithm is used to solve likelihood estimate functions. Experimental results demonstrate that the proposed approach can be used to find fault patterns of gearbox without the prior knowledge of their generated mechanisms.","2021","2024-03-06 13:06:08","2024-05-03 09:40:04","2024-03-06 13:06:08","01052","","","252","","E3S Web Conf.","","","","","","","","en","","","","","DOI.org (Crossref)","","","<div data-schema-version=""8""><p><span style=""color: rgb(31, 31, 31)""><span style=""background-color: rgb(255, 255, 255)"">Shallow learning for bearing faults using Gaussian Mixture Models and maximum likelihood</span></span></p> </div>","/Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/4HDDBTN4/Hu and Huang - 2021 - A Novel Vehicle Gearbox Fault Diagnosis Approach B.pdf; ","notion://www.notion.so/Hu-Huang-2021-e57e846d1469438dbf338e51f4edeb52","Computer Science - Machine Learning; Anomaly detection; notion; Classified; Read","","Rashed, G.I.; Kheshti, M.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VJJNGYSD","journalArticle","2022","Chi, Fulin; Yang, Xinyu; Shao, Siyu; Zhang, Qiang","Bearing Fault Diagnosis for Time-Varying System Using Vibration–Speed Fusion Network Based on Self-Attention and Sparse Feature Extraction","Machines","","2075-1702","10.3390/machines10100948","https://www.mdpi.com/2075-1702/10/10/948","Nowadays, most deep-learning-based bearing fault diagnosis methods are studied under the condition of steady speed, while the performance of these models cannot be fully played under time-varying conditions. Therefore, in order to facilitate the practical application of a deep learning model in bearing fault diagnosis, a vibration–speed fusion network is proposed, which utilizes a transformer with a self-attention module to extract vibration features and utilizes a sparse autoencoder (SAE) network to extract sparse features from speed pulse signal. The vibration–speed fusion network enables the efﬁcient fusion of different signals in a high-dimensional vector space with a high degree of model interpretability, without additional signal processing steps. After tuning the hyperparameters of the network, the key segments of the bearing’s time-domain vibration signals can be optimally extracted, the network performance is much better than traditional deep learning methods, and the classiﬁcation accuracy can reach 95.18% and 99.85% on the two public bearing datasets from the Xi’an Jiaotong University and the University of Ottawa.","2022-10-18","2024-03-20 16:16:03","2024-03-27 14:34:37","2024-03-20 16:16:03","948","","10","10","","Machines","","","","","","","","en","","","","","DOI.org (Crossref)","","","<p style=""margin: 0.0px 0.0px 0.0px 0.0px; font: 13.3px Arial; -webkit-text-stroke: #000000;""><span style=""font-family: 'Arial'; font-weight: normal; font-style: normal; font-size: 13.33px; font-kerning: none;"">Fuses an autoencoder with a transformer for bearing fault classification from speed and vibration data. Multi head attention of transformer makes it more robust to noise. Unclear results although good approach and explanation of the adaptation of transformer to time series</span></p>","/Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/BSAITIZL/Chi et al. - 2022 - Bearing Fault Diagnosis for Time-Varying System Us.pdf; ","notion://www.notion.so/Chi-et-al-2022-1108d9f4dd2f4101b225998de25820d9","Anomaly detection; Deep Learning; Fault classification; Latent space; Historical Data; Transformer; Autoencoder; notion; Data handling techniques","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HMAYULJ4","journalArticle","2020","Kim, Sunuwe; Jo, Soo-Ho; Kim, Wongon; Park, Jongmin; Jeong, Jingyo; Han, Yeongmin; Kim, Daeil; Youn, Byeng Dong","A Semi-Supervised Autoencoder With an Auxiliary Task (SAAT) for Power Transformer Fault Diagnosis Using Dissolved Gas Analysis","IEEE Access","","2169-3536","10.1109/ACCESS.2020.3027830","https://ieeexplore.ieee.org/document/9210082/","This paper proposes a semi-supervised autoencoder with an auxiliary task (SAAT) to extract a health feature space for power transformer fault diagnosis using dissolved gas analysis (DGA). The health feature space generated by a semi-supervised autoencoder (SSAE) not only identiﬁes normal and thermal/electrical fault types, but also presents the underlying characteristics of DGA. In the proposed approach, by adding an auxiliary task that detects normal and fault states in the loss function of SSAE, the health feature space additionally enables visualization of health degradation properties. The overall procedure of the new approach includes three key steps: 1) preprocessing DGA data, 2) extracting two health features via SAAT, and 3) visualizing the two health features in two-dimensional space. In this paper, we test the proposed approach using massive unlabeled/labeled Korea Electric Power Corporation (KEPCO) databases and IEC TC 10 databases. To demonstrate the effectiveness of the proposed approach, four comparative studies are conducted with these datasets; the studies examined: 1) the effectiveness of an auxiliary detection task, 2) the effectiveness of the visualization method, 3) conventional fault diagnosis methods, and 4) the state-of-the-art, semi-supervised deep learning algorithms. By examining several evaluation metrics, these comparative studies conﬁrm that the proposed approach outperforms SSAE without the auxiliary task, existing methods, and state-of-the-art deep learning algorithms, in terms of deﬁning health degradation performance. We expect that the proposed SAAT-based health feature space approach will be widely applicable to intuitively monitor the health state of power transformers in the real world.","2020","2024-03-20 16:16:00","2025-05-25 10:55:44","2024-03-20 16:16:00","178295-178310","","","8","","IEEE Access","","","","","","","","en","","","","","DOI.org (Crossref)","","","<div data-schema-version=""8""><p><span style=""color: rgb(31, 31, 31)""><span style=""background-color: rgb(255, 255, 255)"">Uses Autoencoder for both unsupervised feature extraction (rec loss) and fault classification on healthy/faulty (auxiliary loss, from encoder output). Applies oversampling to overcome imbalanced data. Data from Power transformers. Good example of semi-supervised approach, that uses AE trained encoder to extract features and then being able to classify faults with that trained encoder output</span></span></p> <p></p> </div>","/Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/BDSYYXX4/Kim et al. - 2020 - A Semi-Supervised Autoencoder With an Auxiliary Ta.pdf; ","notion://www.notion.so/Kim-et-al-2020-89571791e2d8434f996042804c01fc35","Anomaly detection; Autoencoder; Classified; Deep Learning; Fault classification; Health degradation properties; Latent space; NN; notion; Oversampling; Read","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VAYD96RA","journalArticle","2021","Thill, Markus; Konen, Wolfgang; Wang, Hao; Bäck, Thomas","Temporal convolutional autoencoder for unsupervised anomaly detection in time series","Applied Soft Computing","","15684946","10.1016/j.asoc.2021.107751","https://linkinghub.elsevier.com/retrieve/pii/S1568494621006724","Learning temporal patterns in time series remains a challenging task up until today. Particularly for anomaly detection in time series, it is essential to learn the underlying structure of a system’s normal behavior. Periodic or quasiperiodic signals with complex temporal patterns make the problem even more challenging: Anomalies may be a hard-to-detect deviation from the normal recurring pattern. In this paper, we present TCN-AE, a temporal convolutional network autoencoder based on dilated convolutions. Contrary to many other anomaly detection algorithms, TCN-AE is trained in an unsupervised manner. The algorithm demonstrates its efficacy on a comprehensive real-world anomaly benchmark comprising electrocardiogram (ECG) recordings of patients with cardiac arrhythmia. TCNAE significantly outperforms several other unsupervised state-of-the-art anomaly detection algorithms. Moreover, we investigate the contribution of the individual enhancements and show that each new ingredient improves the overall performance on the investigated benchmark.","2021-11","2024-03-20 15:54:17","2024-06-30 20:48:11","2024-03-20 15:54:17","107751","","","112","","Applied Soft Computing","","","","","","","","en","","","","","DOI.org (Crossref)","","","<p>Really good application of anomaly detection on time series unsupervised using temporal convolution autoencoder. Really good results tested on ECG data. Really good theory and task explanation as well as architecture analysis</p>","; /Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/HHEBSZL2/Thill et al. - 2021 - Temporal convolutional autoencoder for unsupervise.pdf","notion://www.notion.so/Thill-et-al-2021-90b48ede2f434118bd98f01629130abe","Latent space; Autoencoder; notion; Data handling techniques; 1D input; Classified; Read; CNN; Good task theory (DL); Good Autoencoder theory; Good anomalies type explanation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T49ATRLX","journalArticle","2019","Munir, Mohsin; Siddiqui, Shoaib Ahmed; Chattha, Muhammad Ali; Dengel, Andreas; Ahmed, Sheraz","FuseAD: Unsupervised Anomaly Detection in Streaming Sensors Data by Fusing Statistical and Deep Learning Models","Sensors","","1424-8220","10.3390/s19112451","https://www.mdpi.com/1424-8220/19/11/2451","The need for robust unsupervised anomaly detection in streaming data is increasing rapidly in the current era of smart devices, where enormous data are gathered from numerous sensors. These sensors record the internal state of a machine, the external environment, and the interaction of machines with other machines and humans. It is of prime importance to leverage this information in order to minimize downtime of machines, or even avoid downtime completely by constant monitoring. Since each device generates a different type of streaming data, it is normally the case that a speciﬁc kind of anomaly detection technique performs better than the others depending on the data type. For some types of data and use-cases, statistical anomaly detection techniques work better, whereas for others, deep learning-based techniques are preferred. In this paper, we present a novel anomaly detection technique, FuseAD, which takes advantage of both statistical and deep-learning-based approaches by fusing them together in a residual fashion. The obtained results show an increase in area under the curve (AUC) as compared to state-of-the-art anomaly detection methods when FuseAD is tested on a publicly available dataset (Yahoo Webscope benchmark). The obtained results advocate that this fusion-based technique can obtain the best of both worlds by combining their strengths and complementing their weaknesses. We also perform an ablation study to quantify the contribution of the individual components in FuseAD, i.e., the statistical ARIMA model as well as the deep-learning-based convolutional neural network (CNN) model.","2019-05-29","2024-04-25 15:52:00","2025-04-26 11:01:52","2024-04-25 15:52:00","2451","","11","19","","Sensors","FuseAD","","","","","","","en","https://creativecommons.org/licenses/by/4.0/","","","","DOI.org (Crossref)","","","<div data-schema-version=""9""><p>Good paper on how to integrate statistical classical model (ARIMA) with DL model (CNN) for anomaly detection. Good reference for that. However not for industrial data as uses yahoo login user data and some datasets from NAB. Still, good explanation and results comparison</p> </div>","/Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/6AMYRC2Y/Munir et al. - 2019 - FuseAD Unsupervised Anomaly Detection in Streamin.pdf; ","notion://www.notion.so/Munir-et-al-2019-83bf23b202174b94af2d37bb714df589","Statistics - Machine Learning; Deep Learning; notion; 1D input; Classified; Read; CNN; Good task theory (DL); Potential reference; To read for paper","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K6J2BQH6","journalArticle","2020","Minghu Zhang; Zhang, Minghu; Minghu Zhang; Jianwen Guo; Guo, Jianwen; Jianwen Guo; Xin Li; Li, Xin; Rui Jin; Jin, Rui","Data-Driven Anomaly Detection Approach for Time-Series Streaming Data.","Sensors","","","10.3390/s20195646","","Recently, wireless sensor networks (WSNs) have been extensively deployed to monitor environments. Sensor nodes are susceptible to fault generation due to hardware and software failures in harsh environments. Anomaly detection for the time-series streaming data of sensor nodes is a challenging but critical fault diagnosis task, particularly in large-scale WSNs. The data-driven approach is becoming essential for the goal of improving the reliability and stability of WSNs. We propose a data-driven anomaly detection approach in this paper, named median filter (MF)-stacked long short-term memory-exponentially weighted moving average (LSTM-EWMA), for time-series status data, including the operating voltage and panel temperature recorded by a sensor node deployed in the field. These status data can be used to diagnose device anomalies. First, a median filter (MF) is introduced as a preprocessor to preprocess obvious anomalies in input data. Then, stacked long short-term memory (LSTM) is employed for prediction. Finally, the exponentially weighted moving average (EWMA) control chart is employed as a detector for recognizing anomalies. We evaluate the proposed approach for the panel temperature and operating voltage of time-series streaming data recorded by wireless node devices deployed in harsh field conditions for environmental monitoring. Extensive experiments were conducted on real time-series status data. The results demonstrate that compared to other approaches, the MF-stacked LSTM-EWMA approach can significantly improve the detection rate (DR) and false rate (FR). The average DR and FR values with the proposed approach are 95.46% and 4.42%, respectively. MF-stacked LSTM-EWMA anomaly detection also achieves a better F2 score than that achieved by other methods. The proposed approach provides valuable insights for anomaly detection in WSNs by detecting anomalies in the time-series status data recorded by wireless sensor nodes.","2020","2024-04-25 15:39:16","2024-10-23 15:05:54","","5646","","19","20","","","","","","","","","","","","","","","","","DOI: 10.3390/s20195646 MAG ID: 3090041645 PMCID: 7582627 PMID: 33023175 S2ID: 857113cc4e991a1dae483cc34bc0a1886d258557","<p>Anomaly detection on wireless sensors sthat stream time series data. Preprocesses with rolling median, uses stacked LSTM to predict and the n compare against full read trace while using exponential moving average to determine if segment is anomalous. Mostly detects outliers.</p>","; /Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/8RGFXF74/Zhang et al. - 2020 - Data-Driven Anomaly Detection Approach for Time-Se.pdf","notion://www.notion.so/Minghu-Zhang-et-al-2020-c1e61373a3d54007bca993b556073d16","Anomaly detection; Deep Learning; SCADA data; notion; Data handling techniques; 1D input; Classified; Read; Good task theory (DL); Processed data; LSTM; Clean noise in training data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5NX4G42F","preprint","2023","Yang, Chaocheng; Wang, Tingyin; Yan, Xuanhui","DDMT: Denoising Diffusion Mask Transformer Models for Multivariate Time Series Anomaly Detection","","","","10.48550/arXiv.2310.08800","http://arxiv.org/abs/2310.08800","Anomaly detection in multivariate time series has emerged as a crucial challenge in time series research, with significant research implications in various fields such as fraud detection, fault diagnosis, and system state estimation. Reconstruction-based models have shown promising potential in recent years for detecting anomalies in time series data. However, due to the rapid increase in data scale and dimensionality, the issues of noise and Weak Identity Mapping (WIM) during time series reconstruction have become increasingly pronounced. To address this, we introduce a novel Adaptive Dynamic Neighbor Mask (ADNM) mechanism and integrate it with the Transformer and Denoising Diffusion Model, creating a new framework for multivariate time series anomaly detection, named Denoising Diffusion Mask Transformer (DDMT). The ADNM module is introduced to mitigate information leakage between input and output features during data reconstruction, thereby alleviating the problem of WIM during reconstruction. The Denoising Diffusion Transformer (DDT) employs the Transformer as an internal neural network structure for Denoising Diffusion Model. It learns the stepwise generation process of time series data to model the probability distribution of the data, capturing normal data patterns and progressively restoring time series data by removing noise, resulting in a clear recovery of anomalies. To the best of our knowledge, this is the first model that combines Denoising Diffusion Model and the Transformer for multivariate time series anomaly detection. Experimental evaluations were conducted on five publicly available multivariate time series anomaly detection datasets. The results demonstrate that the model effectively identifies anomalies in time series data, achieving state-of-the-art performance in anomaly detection.","2023-10-30","2024-12-20 11:22:53","2025-05-25 10:54:53","2024-12-20 11:22:53","","","","","","","DDMT","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2310.08800 [cs]","Comment: 16 pages, 9 figures; <div data-schema-version=""9""><p>Good paper on state of the art model using transformers for multivariate time series anomaly detection. Uses an attention mask first and then diffusion transformers to reconstruct the input (similar to our task but using diffusion). Compares against other read SOTA AD models and public datasets (SMD; SWaT)</p> </div>","; /Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/F5WY7T3E/Yang et al. - 2023 - DDMT Denoising Diffusion Mask Transformer Models .pdf","notion://www.notion.so/Yang-et-al-2023-1623c3c2f8f2816fb604e31c08b1ed39","1D input; Anomaly detection; Attention; Autoencoder; Classified; Deep Learning; Good anomalies type explanation; Good Autoencoder theory; Good task theory (DL); Industrial data; Multivariate; notion; Read; state of the art; Transformer","Computer Science - Artificial Intelligence; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2310.08800","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AYIIIJME","conferencePaper","2019","Serrano, Sofia; Smith, Noah A.","Is Attention Interpretable?","Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics","","","10.18653/v1/P19-1282","https://www.aclweb.org/anthology/P19-1282","","2019","2024-12-22 11:40:48","2025-01-03 17:19:09","2024-12-22 11:40:48","2931-2951","","","","","","","","","","","Association for Computational Linguistics","Florence, Italy","en","","","","","DOI.org (Crossref)","","","<div data-schema-version=""9""><p>Paper trying to demythize attention visualization as interpretable. It performs some tests on a text classification task with a known model (HAN). It concludes that sometimes attention weights do correlate with weight impact in final model but not always (difficult to take away something)</p> </div>","; /Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/9552ANU8/Serrano and Smith - 2019 - Is Attention Interpretable.pdf","notion://www.notion.so/Serrano-y-Smith-2019-1643c3c2f8f2814a8325d2081cba9f4e","1D input; Attention; Classified; Deep Learning; Good XAI theory explanation; Interpretability; notion; Read; state of the art","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics","","","","","","","","","","","","","","",""
"366HY59L","journalArticle","2024","Lee, Daesoo; Malacarne, Sara; Aune, Erlend","Explainable time series anomaly detection using masked latent generative modeling","Pattern Recognition","","00313203","10.1016/j.patcog.2024.110826","https://linkinghub.elsevier.com/retrieve/pii/S0031320324005776","We present a novel time series anomaly detection method that achieves excellent detection accuracy while offering a superior level of explainability. Our proposed method, TimeVQVAE-AD, leverages masked generative modeling adapted from the cutting-edge time series generation method known as TimeVQVAE. The prior model is trained on the discrete latent space of a time–frequency domain. Notably, the dimensional semantics of the time–frequency domain are preserved in the latent space, enabling us to compute anomaly scores across different frequency bands, which provides a better insight into the detected anomalies. Additionally, the generative nature of the prior model allows for sampling likely normal states for detected anomalies, enhancing the explainability of the detected anomalies through counterfactuals. Our experimental evaluation on the UCR Time Series Anomaly archive demonstrates that TimeVQVAE-AD significantly surpasses the existing methods in terms of detection accuracy and explainability. We provide our implementation on GitHub: https://github.com/ML4ITS/TimeVQVAE-AnomalyDetection.","2024-12","2024-12-22 11:42:03","2024-12-29 12:26:35","2024-12-22 11:42:03","110826","","","156","","Pattern Recognition","","","","","","","","en","","","","","DOI.org (Crossref)","","","<div data-schema-version=""9""><p>Good recent paper on anomaly detection for time series. Similar example as in our case of study. Leverage TimeVQVAE model already published and improves it for increasing detectability. Uses reconstruction loss as explainability of anomaly (same idea as ours). Good explanation of limitations of other architectures. TimeVQVAE to be investigated as it looks promising. Uses UCR dataset for benchmarking.</p> </div>","/Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/IZ3TEBJ2/Lee et al. - 2024 - Explainable time series anomaly detection using ma.pdf; ","notion://www.notion.so/Lee-et-al-2024-1643c3c2f8f281b88695e19a3370c586","1D input; 2D input; Adds context info; Anomaly detection; Autoencoder; Classified; Data handling techniques; Deep Learning; GAN; Good anomalies type explanation; Good Autoencoder theory; Good task theory (DL); Good time series theory; Industrial data; Interpretability; Latent space; NN; notion; Re-read; Read; state of the art; Transformer; Variable Autoencoder","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NMKQ4ADD","preprint","2023","Hickling, Thomas; Zenati, Abdelhafid; Aouf, Nabil; Spencer, Phillippa","Explainability in Deep Reinforcement Learning, a Review into Current Methods and Applications","","","","10.48550/arXiv.2207.01911","http://arxiv.org/abs/2207.01911","The use of Deep Reinforcement Learning (DRL) schemes has increased dramatically since their first introduction in 2015. Though uses in many different applications are being found, they still have a problem with the lack of interpretability. This has bread a lack of understanding and trust in the use of DRL solutions from researchers and the general public. To solve this problem, the field of Explainable Artificial Intelligence (XAI) has emerged. This entails a variety of different methods that look to open the DRL black boxes, ranging from the use of interpretable symbolic Decision Trees (DT) to numerical methods like Shapley Values. This review looks at which methods are being used and for which applications. This is done to identify which models are the best suited to each application or if a method is being underutilised.","2023-03-07","2024-12-22 11:44:17","2024-12-29 12:07:04","2024-12-22 11:44:17","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2207.01911 [cs]","Comment: 30 pages, 6 figures, Paper Review; <div data-schema-version=""9""><p>Papare that reviews XAI techniques for Reinforcement Learning models. Not much use in time series field but good comparison of models and even use of video games to test xAI methods. Concludes that LIME and SHAP are good options</p> </div>","/Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/V293XTBX/Hickling et al. - 2023 - Explainability in Deep Reinforcement Learning, a R.pdf; ","notion://www.notion.so/Hickling-et-al-2023-1643c3c2f8f281139843d35d6a8ac31b","Classified; Deep Learning; Good XAI theory explanation; Interpretability; notion; Read","Computer Science - Artificial Intelligence; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2207.01911","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XQBDJRUV","journalArticle","2024","Bommer, Philine Lou; Kretschmer, Marlene; Hedström, Anna; Bareeva, Dilyara; Höhne, Marina M.-C.","Finding the Right XAI Method—A Guide for the Evaluation and Ranking of Explainable AI Methods in Climate Science","Artificial Intelligence for the Earth Systems","","2769-7525","10.1175/AIES-D-23-0074.1","https://journals.ametsoc.org/view/journals/aies/3/3/AIES-D-23-0074.1.xml","Explainable artiﬁcial intelligence (XAI) methods shed light on the predictions of machine learning algorithms. Several different approaches exist and have already been applied in climate science. However, usually missing ground truth explanations complicate their evaluation and comparison, subsequently impeding the choice of the XAI method. Therefore, in this work, we introduce XAI evaluation in the climate context and discuss different desired explanation properties, namely, robustness, faithfulness, randomization, complexity, and localization. To this end, we chose previous work as a case study where the decade of annual-mean temperature maps is predicted. After training both a multilayer perceptron (MLP) and a convolutional neural network (CNN), multiple XAI methods are applied and their skill scores in reference to a random uniform explanation are calculated for each property. Independent of the network, we ﬁnd that XAI methods such as Integrated Gradients, layerwise relevance propagation, and input times gradients exhibit considerable robustness, faithfulness, and complexity while sacriﬁcing randomization performance. Sensitivity methods, gradient, SmoothGrad, NoiseGrad, and FusionGrad, match the robustness skill but sacriﬁce faithfulness and complexity for the randomization skill. We ﬁnd architecture-dependent performance differences regarding robustness, complexity, and localization skills of different XAI methods, highlighting the necessity for research task-speciﬁc evaluation. Overall, our work offers an overview of different evaluation properties in the climate science context and shows how to compare and benchmark different explanation methods, assessing their suitability based on strengths and weaknesses, for the speciﬁc research problem at hand. By that, we aim to support climate researchers in the selection of a suitable XAI method.","2024-07","2024-12-22 11:44:23","2024-12-29 11:25:22","2024-12-22 11:44:23","e230074","","3","3","","","","","","","","","","en","http://www.ametsoc.org/PUBSReuseLicenses","","","","DOI.org (Crossref)","","","<div data-schema-version=""9""><p>Paper that evaluates different xAI techniques on climate weather prediction. It defines some metrics to be used for evaluation (like robustness, randomization, localization, faithfulness…). Different field and different data, although similarity on the lack of ground truth explanation.</p> </div>","/Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/2B58C6HH/Bommer et al. - 2024 - Finding the Right XAI Method—A Guide for the Evalu.pdf; ","notion://www.notion.so/Bommer-et-al-2024-1643c3c2f8f281469d71e06dffa2facf","2D input; Deep Learning; Interpretability; notion; Read","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"INHFVG2I","journalArticle","2022","Veerappa, Manjunatha; Anneken, Mathias; Burkart, Nadia; Huber, Marco F.","Validation of XAI explanations for multivariate time series classification in the maritime domain","Journal of Computational Science","","18777503","10.1016/j.jocs.2021.101539","https://linkinghub.elsevier.com/retrieve/pii/S1877750321001976","Due to the lack of explanation towards their internal mechanism, state-of-the-art deep learning-based classifiers are often considered as black-box models. For instance, in the maritime domain, models that classify the types of ships based on their trajectories and other features perform well, but give no further explanation for their predictions. To gain the trust of human operators responsible for critical decisions, the reason behind the clas­ sification is crucial. In this paper, we introduce explainable artificial intelligence (XAI) approaches to the task of classification of ship types. This supports decision-making by providing explanations in terms of the features contributing the most towards the prediction, along with their corresponding time intervals. In the case of the LIME explainer, we adapt the time-slice mapping technique (LimeforTime), while for Shapley additive expla­ nations (SHAP) and path integrated gradient (PIG), we represent the relevance of each input variable to generate a heatmap as an explanation. In order to validate the XAI results, the existing perturbation and sequence analyses for classifiers of univariate time series data is employed for testing and evaluating the XAI explanations on multivariate time series. Furthermore, we introduce a novel evaluation technique to assess the quality of ex­ planations yielded by the chosen XAI method.","2022-02","2024-12-22 11:42:07","2024-12-29 11:07:45","2024-12-22 11:42:07","101539","","","58","","Journal of Computational Science","","","","","","","","en","","","","","DOI.org (Crossref)","","","<div data-schema-version=""9""><p>Paper that evaluates common XAI techniques normally applied to image and text but for multivariate time series (in particular for maritime data). Good explanation of each technique and concludes that DeepSHAP and Gradient SHAP are most robust.</p> </div>","; /Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/LVDQG7L8/Veerappa et al. - 2022 - Validation of XAI explanations for multivariate ti.pdf","notion://www.notion.so/Veerappa-et-al-2022-1643c3c2f8f28159845ccbb424620d46","1D input; Adds context info; Classified; Deep Learning; Good XAI theory explanation; Interpretability; Latent space; Multivariate; notion; Potential reference; Read; state of the art","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LUYSMYGC","journalArticle","2023","Baidya, Ranjai; Jeong, Heon","Anomaly Detection in Time Series Data Using Reversible Instance Normalized Anomaly Transformer","Sensors","","1424-8220","10.3390/s23229272","https://www.mdpi.com/1424-8220/23/22/9272","Anomalies are infrequent in nature, but detecting these anomalies could be crucial for the proper functioning of any system. The rarity of anomalies could be a challenge for their detection as detection models are required to depend on the relations of the datapoints with their adjacent datapoints. In this work, we use the rarity of anomalies to detect them. For this, we introduce the reversible instance normalized anomaly transformer (RINAT). Rooted in the foundational principles of the anomaly transformer, RINAT incorporates both prior and series associations for each time point. The prior association uses a learnable Gaussian kernel to ensure a thorough understanding of the adjacent concentration inductive bias. In contrast, the series association method uses self-attention techniques to speciﬁcally focus on the original raw data. Furthermore, because anomalies are rare in nature, we utilize normalized data to identify series associations and employ non-normalized data to uncover prior associations. This approach enhances the modelled series associations and, consequently, improves the association discrepancies.","2023-11-19","2024-12-16 11:52:26","2025-04-26 11:24:41","2024-12-16 11:52:26","9272","","22","23","","Sensors","","","","","","","","en","https://creativecommons.org/licenses/by/4.0/","","","","DOI.org (Crossref)","","","<div data-schema-version=""9""><p>Evolution on the Anomaly Transformer paper by adding a normalization and de-normalization technique. Acknowledges it is not the best model which is rare but a good thing. Uses Anomaly Transformer code for performance comparison and some of the popular datasets</p> </div>","/Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/IWVWM7ZR/Baidya and Jeong - 2023 - Anomaly Detection in Time Series Data Using Revers.pdf; ","notion://www.notion.so/Baidya-y-Jeong-2023-15e3c3c2f8f281ccbe0de83a644970c3","Anomaly detection; Transformer; notion; 1D input; Classified; Read; Attention; Good Autoencoder theory; state of the art; Good time series theory; Multivariate; To read for paper","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VXNSYQZL","conferencePaper","2024","Nam, Youngeun; Yoon, Susik; Shin, Yooju; Bae, Minyoung; Song, Hwanjun; Lee, Jae-Gil; Lee, Byung Suk","Breaking the Time-Frequency Granularity Discrepancy in Time-Series Anomaly Detection","Proceedings of the ACM Web Conference 2024","979-8-4007-0171-9","","10.1145/3589334.3645556","https://dl.acm.org/doi/10.1145/3589334.3645556","In light of the remarkable advancements made in time-series anomaly detection (TSAD), recent emphasis has been placed on exploiting the frequency domain as well as the time domain to address the difficulties in precisely detecting pattern-wise anomalies. However, in terms of anomaly scores, the window granularity of the frequency domain is inherently distinct from the data-point granularity of the time domain. Owing to this discrepancy, the anomaly information in the frequency domain has not been utilized to its full potential for TSAD. In this paper, we propose a TSAD framework, Dual-TF , that simultaneously uses both the time and frequency domains while breaking the time-frequency granularity discrepancy. To this end, our framework employs nested-sliding windows, with the outer and inner windows responsible for the time and frequency domains, respectively, and aligns the anomaly scores of the two domains. As a result of the high resolution of the aligned scores, the boundaries of pattern-wise anomalies can be identified more precisely. In six benchmark datasets, our framework outperforms state-of-the-art methods by 12.0–147%, as demonstrated by experimental results.","2024-05-13","2024-12-14 12:46:44","2024-12-15 14:20:45","2024-12-14 12:46:44","4204-4215","","","","","","","","","","","ACM","Singapore Singapore","en","","","","","DOI.org (Crossref)","","","<div data-schema-version=""9""><p>Provides nested sliding windows for the TSAD approach to combine both time and frequency domains. Uses Anomaly Transformer as a model and trains for both reconstructing time and reconstructing frequency per each nested window. Then uses reconstruction error on both to determine if anomaly. Good paper and well explained. Compares against other SOTA architectures.</p> </div>","/Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/42N58CHA/Nam et al. - 2024 - Breaking the Time-Frequency Granularity Discrepanc.pdf; ","notion://www.notion.so/Nam-et-al-2024-15c3c3c2f8f2817a913ec36648e2b29b","Anomaly detection; Deep Learning; Transformer; notion; 1D input; Classified; Read; Attention; Good task theory (DL); Re-read; Good anomalies type explanation; state of the art; Good time series theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","WWW '24: The ACM Web Conference 2024","","","","","","","","","","","","","","",""
"R8SS89MN","journalArticle","2023","Li, Yifan; Peng, Xiaoyan; Zhang, Jia; Li, Zhiyong; Wen, Ming","DCT-GAN: Dilated Convolutional Transformer-Based GAN for Time Series Anomaly Detection","IEEE Transactions on Knowledge and Data Engineering","","1041-4347, 1558-2191, 2326-3865","10.1109/TKDE.2021.3130234","https://ieeexplore.ieee.org/document/9626552/","Time series anomaly detection (TSAD) is an essential problem faced in several ﬁelds, e.g., fault detection, fraud detection, and intrusion detection, etc. Although TSAD is a crucial problem in anomaly detection, few solutions in anomaly detection are suitable for it at present. Recently, some researchers use GAN-based methods such as TAnoGAN and TadGAN to solve TSAD problem.","2023-04-01","2024-12-14 12:46:38","2024-12-15 14:08:47","2024-12-14 12:46:38","3632-3644","","4","35","","IEEE Trans. Knowl. Data Eng.","DCT-GAN","","","","","","","en","https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html","","","","DOI.org (Crossref)","","","<div data-schema-version=""9""><p>Paper proposes a GAN-based framework for TSAD. Uses several generators with a DCT block (dilated convolution + transformer encoder block) and a discriminator in the training stage until Nash equilibrium. Dilated convolution in theory to get more receptive field and not lose resolution. Then AD stage after training to use a single generator and a discriminator to provide anomaly score. Good state of the art paper.</p> </div>","/Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/ILVTUEGE/Li et al. - 2023 - DCT-GAN Dilated Convolutional Transformer-Based G.pdf; ","notion://www.notion.so/Li-et-al-2023-15c3c3c2f8f28100b090f5a2dccd6aa8","Anomaly detection; Deep Learning; Transformer; notion; 1D input; Classified; Read; Attention; GAN; Good task theory (DL); Re-read; Good time series theory; Adversarial training","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5KPVV5XB","preprint","2022","Xu, Jiehui; Wu, Haixu; Wang, Jianmin; Long, Mingsheng","Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy","","","","10.48550/arXiv.2110.02642","http://arxiv.org/abs/2110.02642","Unsupervised detection of anomaly points in time series is a challenging problem, which requires the model to derive a distinguishable criterion. Previous methods tackle the problem mainly through learning pointwise representation or pairwise association, however, neither is sufﬁcient to reason about the intricate dynamics. Recently, Transformers have shown great power in uniﬁed modeling of pointwise representation and pairwise association, and we ﬁnd that the self-attention weight distribution of each time point can embody rich association with the whole series. Our key observation is that due to the rarity of anomalies, it is extremely difﬁcult to build nontrivial associations from abnormal points to the whole series, thereby, the anomalies’ associations shall mainly concentrate on their adjacent time points. This adjacent-concentration bias implies an association-based criterion inherently distinguishable between normal and abnormal points, which we highlight through the Association Discrepancy. Technically, we propose the Anomaly Transformer with a new Anomaly-Attention mechanism to compute the association discrepancy. A minimax strategy is devised to amplify the normal-abnormal distinguishability of the association discrepancy. The Anomaly Transformer achieves state-of-theart results on six unsupervised time series anomaly detection benchmarks of three applications: service monitoring, space & earth exploration, and water treatment.","2022-06-29","2024-12-14 12:46:42","2024-12-15 13:52:41","2024-12-14 12:46:42","","","","","","","Anomaly Transformer","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2110.02642 [cs]","<div data-schema-version=""9""><p>Good work on unsupervised anomaly detection using transformers in time series. I have not fully understood the model but it modifies the vanilla transformer to be able to distinguish anomalies (in each form) by learning to model an association discrepancy both locally and with respect to the whole series. Tests in known benchmarks.</p> </div>","; /Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/ZMYMNSRT/Xu et al. - 2022 - Anomaly Transformer Time Series Anomaly Detection.pdf","notion://www.notion.so/Xu-et-al-2022-15c3c3c2f8f281e6a35bfbe0e1a7bb87","Anomaly detection; Deep Learning; Transformer; notion; 1D input; Classified; Read; Attention; Good task theory (DL); Re-read; Good anomalies type explanation; Good time series theory","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2110.02642","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LZYAZ5YL","preprint","2022","Tuli, Shreshth; Casale, Giuliano; Jennings, Nicholas R.","TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data","","","","10.48550/arXiv.2201.07284","http://arxiv.org/abs/2201.07284","Efficient anomaly detection and diagnosis in multivariate timeseries data is of great importance for modern industrial applications. However, building a system that is able to quickly and accurately pinpoint anomalous observations is a challenging problem. This is due to the lack of anomaly labels, high data volatility and the demands of ultra-low inference times in modern applications. Despite the recent developments of deep learning approaches for anomaly detection, only a few of them can address all of these challenges. In this paper, we propose TranAD, a deep transformer network based anomaly detection and diagnosis model which uses attentionbased sequence encoders to swiftly perform inference with the knowledge of the broader temporal trends in the data. TranAD uses focus score-based self-conditioning to enable robust multi-modal feature extraction and adversarial training to gain stability. Additionally, model-agnostic meta learning (MAML) allows us to train the model using limited data. Extensive empirical studies on six publicly available datasets demonstrate that TranAD can outperform state-of-the-art baseline methods in detection and diagnosis performance with data and time-efficient training. Specifically, TranAD increases F1 scores by up to 17%, reducing training times by up to 99% compared to the baselines.","2022-05-14","2024-12-05 12:13:50","2024-12-08 12:56:16","2024-12-05 12:13:50","","","","","","","TranAD","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2201.07284 [cs]","Comment: Accepted in VLDB 2022; <div data-schema-version=""9""><p><span style=""color: rgb(0, 0, 0)"">Really good paper that uses transformers for anomaly detection in multivariate time series with an adversarial training (autoencoder architecture but with 2 decoders) in 2 phases. First phase is reconstruction using L2 (MSE). 2nd phase is using deviations as input too to highlight deviations. It uses the complete sequence until current timestamp + a window. As preprocessing uses normalization with min-max scaling. Then divides sequences into sliding windows (with padding to maintain window length) for faster training and inference. That way it has 'knowledge' of the full sequence + local information of the window. It is then predicting an anomaly score per each window. It is also reconstructing the time series per window and using the deviation from the original window as what it calls a focus score to feed the encoder again</span></p> </div>","; /Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/WJ22D2NV/Tuli et al. - 2022 - TranAD Deep Transformer Networks for Anomaly Dete.pdf","notion://www.notion.so/Tuli-et-al-2022-1533c3c2f8f281e1b0e9dda7a2b8d1f2","Anomaly detection; Deep Learning; Fault classification; Latent space; Transformer; Autoencoder; notion; Data handling techniques; 1D input; Classified; Read; Attention; Good task theory (DL); Good Autoencoder theory; Re-read; state of the art; Good time series theory; Industrial data; Multivariate; Interpretability; Adversarial training","","","","","","","","","","","","","","","","","","","","arXiv:2201.07284","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8GUCTSXX","journalArticle","2023","Wu, Bo; Fang, Chao; Yao, Zhenjie; Tu, Yanhui; Chen, Yixin","Decompose Auto-Transformer Time Series Anomaly Detection for Network Management","Electronics","","2079-9292","10.3390/electronics12020354","https://www.mdpi.com/2079-9292/12/2/354","Time series anomaly detection through unsupervised methods has been an active research area in recent years due to its enormous potential for networks management. The representation and reconstruction of time series have made extraordinary progress in existing works. However, time series is known to be complex in terms of their temporal dependency and stochasticity, which makes anomaly detection difficult. To this end, we propose a novel approach based on a decomposition auto-transformer networks(DATN) for time series anomaly detection. The time series is decomposed into seasonal and trend components, and renovated as a basic inner block deep model. With this design, transformers can decompose complex time series in a progressive manner. We also design an auto-transfomer block that determines dependencies and representation aggregation at the sub-series level based on series seasonal and trend components. Moreover, the complex transformer decoder is replaced by a simple linear decoder, which makes the model more efficient. Extensive experiments on various public benchmarks demonstrate that our method has achieved state-of-the-art performance.","2023-01-10","2024-12-05 12:13:45","2025-05-01 15:41:45","2024-12-05 12:13:45","354","","2","12","","Electronics","","","","","","","","en","https://creativecommons.org/licenses/by/4.0/","","","","DOI.org (Crossref)","","","<div data-schema-version=""9""><p>Case similar to ours (uses popular datasets and state of the art models like TranAD, OmniAnomaly…). Autoencoder using a modified version of the attention module. It decomposes the series to look for seasonality and trends. It is a reconstruction task too. From the transformer uses the self attention and the multihead attention. Modifies the self attention to feed it first with an FFT (so K, Q, V are calculated from the FFT transformation). Decomposes series into seasonal and trend patterns (using moving average to separate them) and then it feeds each to a branch of the model (for later concatenation and reconstruction). The idea is that one branch captures periodic patterns and the other overall trends. Interesting idea. </p> </div>","; /Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/SZWU6SLZ/Wu et al. - 2023 - Decompose Auto-Transformer Time Series Anomaly Det.pdf","notion://www.notion.so/Wu-et-al-2023-1533c3c2f8f2819baffacf159177e361","Anomaly detection; Deep Learning; Transformer; Autoencoder; notion; Data handling techniques; 1D input; Classified; Read; Attention; Good task theory (DL); state of the art; Industrial data; To read for paper","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6PGWKWJF","preprint","2024","Najafi, Seyed Amirhossein; Asemani, Mohammad Hassan; Setoodeh, Peyman","Attention and Autoencoder Hybrid Model for Unsupervised Online Anomaly Detection","","","","10.48550/arXiv.2401.03322","http://arxiv.org/abs/2401.03322","This paper introduces a hybrid attention and autoencoder (AE) model for unsupervised online anomaly detection in time series. The autoencoder captures local structural patterns in short embeddings, while the attention model learns long-term features, facilitating parallel computing with positional encoding. Unique in its approach, our proposed hybrid model combines attention and autoencoder for the first time in time series anomaly detection. It employs an attention-based mechanism, akin to the deep transformer model, with key architectural modifications for predicting the next time step window in the autoencoder’s latent space. The model utilizes a threshold from the validation dataset for anomaly detection and introduces an alternative method based on analyzing the first statistical moment of error, improving accuracy without dependence on a validation dataset. Evaluation on diverse real-world benchmark datasets and comparing with other well-established models, confirms the effectiveness of our proposed model in anomaly detection.","2024-01-06","2024-12-05 12:13:43","2025-04-26 10:03:38","2024-12-05 12:13:43","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2401.03322 [cs]","<div data-schema-version=""9""><p>Good paper similar to our task. Combines autoencoder and transformer for unsupervised anomaly detection on time series. Trains first autoencoder on windows of time series to predict next windows of time series using Linear hidden layers. After that, trains transformer on latent space encoded windows to predict next encoded window for the decoder to decode. Difficult to understand how the transformer is helping here, supposedly on capturing long term relationships but it is not demonstrated. Checked against some NAB datasets but not considering FN.</p> </div>","/Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/PLNRSSZT/Najafi et al. - 2024 - Attention and Autoencoder Hybrid Model for Unsuper.pdf; ","notion://www.notion.so/Najafi-et-al-2024-1533c3c2f8f2812e9ca6fae04317c91b","Anomaly detection; Deep Learning; Transformer; Autoencoder; notion; 1D input; Classified; Read; Attention; Good task theory (DL); Good Autoencoder theory; Good anomalies type explanation; state of the art; Potential reference; To read for paper","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","arXiv:2401.03322","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F4KUXFNQ","preprint","2022","Neupane, Subash; Fernandez, Ivan A.; Patterson, Wilson; Mittal, Sudip; Rahimi, Shahram","A Temporal Anomaly Detection System for Vehicles utilizing Functional Working Groups and Sensor Channels","","","","10.48550/arXiv.2209.06828","http://arxiv.org/abs/2209.06828","A modern vehicle ﬁtted with sensors, actuators, and Electronic Control Units (ECUs) can be divided into several operational subsystems called Functional Working Groups (FWGs). Examples of these FWGs include the engine system, transmission, fuel system, brakes, etc. Each FWG has associated sensorchannels that gauge vehicular operating conditions. This data rich environment is conducive to the development of Predictive Maintenance (PdM) technologies. Undercutting various PdM technologies is the need for robust anomaly detection models that can identify events or observations which deviate signiﬁcantly from the majority of the data and do not conform to a well deﬁned notion of normal vehicular operational behavior. In this paper, we introduce the Vehicle Performance, Reliability, and Operations (VePRO) dataset and use it to create a multi-phased approach to anomaly detection. Utilizing Temporal Convolution Networks (TCN), our anomaly detection system can achieve 96% detection accuracy and accurately predicts 91% of true anomalies. The performance of our anomaly detection system improves when sensor channels from multiple FWGs are utilized.","2022-09-14","2025-03-07 17:28:17","2025-03-09 12:02:17","2025-03-07 17:28:17","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2209.06828 [cs]","<div data-schema-version=""9""><p>Good paper on time series anomaly detection on sensor data of vehicles. Uses TCN for forecasting windows of sequences (Seq2Seq approach with forecasting). Really interesting dataset (published?) although inserted anomalies. Unclear how they label metrics but still interesting approach. Good task explanation. Really similar problem and approach. Although seems only one vehicle data? (Single stream)</p> </div>","/Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/TIKKAKY4/Neupane et al. - 2022 - A Temporal Anomaly Detection System for Vehicles u.pdf; ","notion://www.notion.so/Neupane-et-al-2022-1af3c3c2f8f281378933d3a56ae77da8","Anomaly detection; notion; Data handling techniques; 1D input; Classified; Read; Good task theory (DL); Good theory explanation (CBM); Good theory explanation (PHM); NN; Re-read; state of the art; Good time series theory; Potential reference","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","arXiv:2209.06828","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DS67HJ4Z","journalArticle","2021","Liu, Yi; Garg, Sahil; Nie, Jiangtian; Zhang, Yang; Xiong, Zehui; Kang, Jiawen; Hossain, M. Shamim","Deep Anomaly Detection for Time-series Data in Industrial IoT: A Communication-Efficient On-device Federated Learning Approach","IEEE Internet of Things Journal","","2327-4662, 2372-2541","10.1109/JIOT.2020.3011726","http://arxiv.org/abs/2007.09712","Since edge device failures (i.e., anomalies) seriously affect the production of industrial products in Industrial IoT (IIoT), accurately and timely detecting anomalies is becoming increasingly important. Furthermore, data collected by the edge device may contain the user’s private data, which is challenging the current detection approaches as user privacy is calling for the public concern in recent years. With this focus, this paper proposes a new communication-efﬁcient ondevice federated learning (FL)-based deep anomaly detection framework for sensing time-series data in IIoT. Speciﬁcally, we ﬁrst introduce a FL framework to enable decentralized edge devices to collaboratively train an anomaly detection model, which can improve its generalization ability. Second, we propose an Attention Mechanism-based Convolutional Neural NetworkLong Short Term Memory (AMCNN-LSTM) model to accurately detect anomalies. The AMCNN-LSTM model uses attention mechanism-based CNN units to capture important ﬁne-grained features, thereby preventing memory loss and gradient dispersion problems. Furthermore, this model retains the advantages of LSTM unit in predicting time series data. Third, to adapt the proposed framework to the timeliness of industrial anomaly detection, we propose a gradient compression mechanism based on Top-k selection to improve communication efﬁciency. Extensive experiment studies on four real-world datasets demonstrate that the proposed framework can accurately and timely detect anomalies and also reduce the communication overhead by 50% compared to the federated learning framework that does not use gradient compression scheme.","2021-04-15","2025-03-07 17:28:22","2025-03-09 12:30:10","2025-03-07 17:28:22","6348-6358","","8","8","","IEEE Internet Things J.","Deep Anomaly Detection for Time-series Data in Industrial IoT","","","","","","","en","","","","","arXiv.org","","arXiv:2007.09712 [cs]","Comment: IEEE Internet of Things Journal; <div data-schema-version=""9""><p>Good paper on applying anomaly detection for IIoT on multi device. Uses attention + CNN + LSTM with time series forecasting to predict sequences. Focused on Federated Learning to preserve privacy. Good task (PdM) explanation and similar problem approach. Also applies gradient compression to minimize computation cost. Good focus on application and reducing computation cost.</p> </div>","/Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/XV8GPR5I/Liu et al. - 2021 - Deep Anomaly Detection for Time-series Data in Ind.pdf; ","notion://www.notion.so/Liu-et-al-2021-1af3c3c2f8f2819c9527dc71eca8db7d","Anomaly detection; Deep Learning; notion; 1D input; Classified; Read; Attention; CNN; Good task theory (DL); Good theory explanation (CBM); Good theory explanation (PHM); LSTM; Re-read; state of the art; Industrial data; Time series forecasting","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Distributed, Parallel, and Cluster Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7XQJNCBB","journalArticle","","Shipmon, Dominique T; Gurevitch, Jason M; Piselli, Paolo M; Edwards, Steve","Time Series Anomaly Detection","","","","","","Google uses continuous streams of data from industry partners in order to deliver accurate results to users. Unexpected drops in traffic can be an indication of an underlying issue and may be an early warning that remedial action may be necessary. Detecting such drops is non-trivial because streams are variable and noisy, with roughly regular spikes (in many different shapes) in traffic data. We investigated the question of whether or not we can predict anomalies in these data streams. Our goal is to utilize Machine Learning and statistical approaches to classify anomalous drops in periodic, but noisy, traffic patterns. Since we do not have a large body of labeled examples to directly apply supervised learning for anomaly classification, we approached the problem in two parts. First we used TensorFlow to train our various models including DNNs, RNNs, and LSTMs to perform regression and predict the expected value in the time series. Secondly we created anomaly detection rules that compared the actual values to predicted values. Since the problem requires finding sustained anomalies, rather than just short delays or momentary inactivity in the data, our two detection methods focused on continuous sections of activity rather than just single points. We tried multiple combinations of our models and rules and found that using the intersection of our two anomaly detection methods proved to be an effective method of detecting anomalies on almost all of our models. In the process we also found that not all data fell within our experimental assumptions, as one data stream had no periodicity, and therefore no time based model could predict it.","","2025-03-07 17:28:12","2025-03-09 11:45:34","","","","","","","","","","","","","","","en","","","","","Zotero","","","<div data-schema-version=""9""><p>Good paper from Google on anomaly detection in time series using ML techniques on highly periodic data. Tests several models but transformers (who should excel in highly periodic data). Single stream of data though. Do not seem to publish dataset. Still good review of models capabilities and comparison with baseline statistical models</p> </div>","; /Users/Adri/Library/CloudStorage/GoogleDrive-adria.molero@upc.edu/Mi unidad/Zotero/storage/M9EPNJFT/Shipmon et al. - Time Series Anomaly Detection.pdf","notion://www.notion.so/Shipmon-et-al-n-d-1b13c3c2f8f281eca9eff0bea9f968e7","Deep Learning; notion; 1D input; Read; Good task theory (DL); NN; state of the art; Computer Science - Artificial Intelligence; Good time series theory; Potential reference","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""